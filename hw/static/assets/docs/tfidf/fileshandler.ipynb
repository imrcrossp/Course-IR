{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7FSKEkq97hQR",
    "outputId": "eb801d09-254d-40c0-d567-74ca3c25edba"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import string\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adamt90WbDtH"
   },
   "source": [
    "### Text_Prepare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "4t7t3v1Y777Q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iir\\Desktop\\code\\tfidf\\20\\\n"
     ]
    }
   ],
   "source": [
    "target_dir = '\\\\20\\\\'\n",
    "base_path = os.getcwd()\n",
    "print(base_path+target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "J_ojPS3S8GpF"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "def para_prepare(strr):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    originals = nltk.sent_tokenize(strr)\n",
    "    REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "    BAD_SYMBOLS_RE = re.compile('[^a-z\\ ]')\n",
    "    STOPWORDS = set(nltk.corpus.stopwords.words('english'))\n",
    "    results = []\n",
    "    for pos, i in enumerate(originals):\n",
    "        i = i.lower()\n",
    "        i = REPLACE_BY_SPACE_RE.sub(\" \", i)\n",
    "        i = BAD_SYMBOLS_RE.sub(\"\", i)\n",
    "        sent = \" \".join([w for w in nltk.word_tokenize(i) if w not in STOPWORDS]) \n",
    "        results.append(sent)\n",
    "        \n",
    "    return results, originals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "WXQ31RxYbVmI"
   },
   "outputs": [],
   "source": [
    "class MyCorpus:\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.files = os.listdir(path)\n",
    "        self.fn = \"\"\n",
    "        \n",
    "    def getfn(self):\n",
    "        return self.fn\n",
    "    \n",
    "    def __iter__(self):    \n",
    "        for fn in self.files:\n",
    "            _data = pd.read_csv(self.path+'/'+fn)\n",
    "            self.fn = self.path+'/'+fn\n",
    "            data_size = len(_data)\n",
    "            for i in range(data_size):\n",
    "                top = _data['Title'][i]\n",
    "                para = _data['Abstract'][i]\n",
    "                \"\"\"Check abstract is null or not.\"\"\"\n",
    "                if type(para) == float and math.isnan(para): \n",
    "                    continue\n",
    "                sent, ori = para_prepare(para)\n",
    "                for stc, o in zip(sent, ori):\n",
    "                    if stc == \"\":\n",
    "                        continue\n",
    "                    yield stc, o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1739\n"
     ]
    }
   ],
   "source": [
    "sents = MyCorpus(base_path+target_dir)\n",
    "i = 0\n",
    "for sent, j in sents:\n",
    "    i = i + 1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oYwtwPChoGfX"
   },
   "source": [
    "### Runingggg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "07UcyOEuIEX1"
   },
   "outputs": [],
   "source": [
    "sentences = MyCorpus(base_path+target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "FoeRJXD_3VbR"
   },
   "outputs": [],
   "source": [
    "replace_dict = {\n",
    "    \"hepatitis b\": \"hepatitisb\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_dict(sents, re_dict, porter = False):\n",
    "    words = {}\n",
    "    total_words = 0\n",
    "    total_sents = 0\n",
    "    for i, j in sents:\n",
    "        new_sent = True\n",
    "        for _old, new_ in re_dict.items():\n",
    "            i = i.replace(_old, new_)\n",
    "        token = nltk.word_tokenize(i)\n",
    "        for pos, t in enumerate(token):\n",
    "            for _old, new_ in re_dict.items():\n",
    "                if t == new_:\n",
    "                    token[pos] = _old\n",
    "        if porter == False:\n",
    "            for i in token:\n",
    "                if i in words:\n",
    "                    words[i] = words[i] + 1\n",
    "                else:\n",
    "                    words[i] = 1\n",
    "                total_words = total_words + 1\n",
    "        else:\n",
    "            stemmer = PorterStemmer()\n",
    "            for i in token:\n",
    "                i = stemmer.stem(i)\n",
    "                if i in words:\n",
    "                    words[i] = words[i] + 1\n",
    "                else:\n",
    "                    words[i] = 1\n",
    "                total_words = total_words + 1\n",
    "        total_sents = total_sents + 1\n",
    "    words = dict(collections.OrderedDict(sorted(words.items())))\n",
    "    return words, total_words, total_sents\n",
    "    # return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_nat = lambda x, y: x/y\n",
    "tf_log = lambda x, y: math.log(1+x)\n",
    "tf_aug = lambda x, y: 0.5+0.5*((x)/(y))\n",
    "tf_avg = lambda x, y: ((1+math.log(x))/(1+math.log(y)))\n",
    "'''-------------------------------------------'''\n",
    "idf_nat = lambda x, y: math.log(x/y)\n",
    "idf_smt = lambda x, y: math.log(x/(1+y))+1\n",
    "idf_max = lambda x, y: math.log(x/(1+y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, tw, ts = words_dict(MyCorpus(base_path+target_dir), replace_dict, porter = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = MyCorpus(base_path+target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "results = {\"processed\": [], \"nonprocessed\": []}\n",
    "for i, j in corpus:\n",
    "    results[\"processed\"].append(i)\n",
    "    results[\"nonprocessed\"].append(j)\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "results_df.to_csv(\"./book.csv\", header=True, mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "results = {\"keys\": []}\n",
    "for key in words.keys():\n",
    "    results[\"keys\"].append(key)\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"./keys.csv\", header=True, mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "''' if you have dict file, please change some code'''\n",
    "replace_dict = {\n",
    "    \"hepatitis b\": \"hepatitisb\",\n",
    "}\n",
    "keys_df = pd.read_csv(\"./keys.csv\")\n",
    "keys = keys_df[\"keys\"].tolist()\n",
    "dummy = []\n",
    "results = {'_keys': keys}\n",
    "# for key in words.keys():\n",
    "#     results[\"keys\"].append(key)\n",
    "# results_df = pd.DataFrame(results)\n",
    "corpus = MyCorpus(base_path+target_dir)\n",
    "idx = 0\n",
    "for i, useless in corpus:\n",
    "    results[idx] =  [0] * len(keys)\n",
    "    \n",
    "    for _old, new_ in replace_dict.items():\n",
    "        ir = i.replace(_old, new_)\n",
    "    token = nltk.word_tokenize(ir)\n",
    "    for pos, t in enumerate(token):\n",
    "            for _old, new_ in replace_dict.items():\n",
    "                if t == new_:\n",
    "                    token[pos] = _old\n",
    "    for j in token:\n",
    "        results[idx][keys.index(j)] = results[idx][keys.index(j)] + 1\n",
    "    idx = idx + 1\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"./idf.csv\", header=True, mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "replace_dict = {\n",
    "    \"hepatitis b\": \"hepatitisb\",\n",
    "}\n",
    "keys = [i for i, j in words.items()]\n",
    "dummy = ['_tf', '_ts', '_tw']+keys\n",
    "results = {'terms': dummy, 'freq': [tw, ts, len(keys)]}\n",
    "corpus = MyCorpus(base_path+target_dir)\n",
    "for key, freq in words.items():\n",
    "    results['freq'].append(freq)\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"./tf.csv\", header=True, mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "prepare_train.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
